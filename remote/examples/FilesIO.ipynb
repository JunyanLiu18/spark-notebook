{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize SparkContext\n",
    "\n",
    "The `master_url` variable contains the URL of the Spark master node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext(master=master_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local files\n",
    "\n",
    "## Download and Uplaod\n",
    "\n",
    "Download and upload files via the Spark Notebook interface.\n",
    "\n",
    "## Access Local Files\n",
    "\n",
    "The file path to local files requires `file://` prefix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "core-site.xml                slaves.template\r\n",
      "docker.properties.template   spark-defaults.conf\r\n",
      "fairscheduler.xml.template   spark-defaults.conf.template\r\n",
      "log4j.properties.template    \u001b[0m\u001b[01;32mspark-env.sh\u001b[0m*\r\n",
      "metrics.properties.template  \u001b[01;32mspark-env.sh.template\u001b[0m*\r\n",
      "slaves\r\n",
      "\u001b[m"
     ]
    }
   ],
   "source": [
    "ls /root/spark/conf/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'ec2-54-88-27-130.compute-1.amazonaws.com']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_files = sc.textFile(\"file:///root/spark/conf/slaves\")\n",
    "local_files.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S3 files\n",
    "\n",
    "The object `s3helper` is created to help you access S3 files.\n",
    "\n",
    "Run `help(s3helper)` to learn all its methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "help(s3helper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) Set AWS Credentials\n",
    "\n",
    "To access a S3 bucket, the first step is to set AWS credential. There are two ways to do it.\n",
    "\n",
    "1. (**RECOMMENDED**) Set S3 credentials via Spark Notebook interface.\n",
    "2. Set it using the `set_credential` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s3helper.set_credential(AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) Open the bucket that has your files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s3helper.open_bucket('your-bucket-name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) List files in the bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print s3helper.ls()  # By default, list all files in the root directory of the bucket\n",
    "print s3helper.ls('sub-directory')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (4) Read files from S3\n",
    "\n",
    "There are two ways to read files from S3.\n",
    "\n",
    "**(1) Get the list of S3 file paths and pass it to Spark. Spark supports read files directly from S3.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file_paths = s3helper.get_path('/sub-directory')\n",
    "rdd = sc.textFile(','.join(file_paths))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** (2) Load S3 files to the HDFS on this cluster and read them from HDFS **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "files = s3helper.load_path('/sub-directory', '/hdfs-directory')\n",
    "rdd = sc.textFile(','.join(files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parquet Files\n",
    "\n",
    "To get a reasonable reading speed, please always load parquet files from S3 to HDFS before accessing them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s3helper.open_bucket(\"your-bucket-name\")\n",
    "\n",
    "files = s3helper.load_path('/sub-directory-for-parquets', '/hdfs-directory.parquet')\n",
    "files[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "sqlContext = SQLContext(sc)\n",
    "df = sqlContext.sql(\"SELECT key, value FROM parquet.`/hdfs-directory.parquet`\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
